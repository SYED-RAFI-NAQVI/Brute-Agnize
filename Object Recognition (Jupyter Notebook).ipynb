{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Near State-of-the-Art results at Object Recognition\n",
    "### Presented by Eduonix\n",
    "\n",
    "In this project, we will be deploying a convolutional neural network (CNN) for object recognition. More specifically, we will be using the All-CNN network published in the 2015 ICLR paper, \"Striving For Simplicity: The All Convolutional Net\".  This paper can be found at the following link:\n",
    "\n",
    "https://arxiv.org/pdf/1412.6806.pdf\n",
    "\n",
    "This convolutional neural network obtained state-of-the-art performance at object recognition on the CIFAR-10 image dataset in 2015. We will build this model using Keras, a high-level neural network application programming interface (API) that supports both Theano and Tensorflow backends. You can use either backend; however, I will be using Theano.  \n",
    "\n",
    "In this project, we will learn to:\n",
    "* Import datasets from Keras\n",
    "* Use one-hot vectors for categorical labels\n",
    "* Addlayers to a Keras model\n",
    "* Load pre-trained weights\n",
    "* Make predictions using a trained Keras model\n",
    "\n",
    "The dataset we will be using is the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "### 1. Loading the Data\n",
    "\n",
    "Let's dive right in! In these first few cells, we will import necessary packages, load the dataset, and plot some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images: (50000, 32, 32, 3)\n",
      "Testing Images: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets determine the dataset characteristics\n",
    "print('Training Images: {}'.format(X_train.shape))\n",
    "print('Testing Images: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Now for a single image \n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a43e332f8602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m330\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# show the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2701\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5492\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5494\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5495\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    644\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    645\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAG0AAABjCAYAAACR8o4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABKZJREFUeJztnUFoXFUUhr/f1CpkYcFmIVqoxWLIwkU6SFYiiNB2kSx0kW5qpBKKFteCC6EbcSUUxVKxaF3UYlcRFEEUumrtBLS2ipIKYjDQVCUboRo4Lt5LMk4mmZvxvrwccj4YmDf3vntP8vEmb07m3CszI/DFXXUHEGyckOaQkOaQkOaQkOaQkOaQrtIknZV0S9L1Ndol6ZSkGUnXJA3nDzNoJeVKex84uE77IWB/+ZgE3vn/YQXr0VWamV0C/linyxhwzgouA7skPZArwGA1Of6mPQj82nI8W74WVMSODGOow2sdc2OSJineQunv7z8wODiYYXqfTE9P3zazgV7OzSFtFtjTcvwQ8FunjmZ2BjgD0Gg0rNlsZpjeJ5J+6fXcHG+PU8DR8i5yBFgws7kM4wZr0PVKk3QeeBLYLWkWeA24G8DMTgOfAoeBGeAv4Pmqgg0KukozsyNd2g14KVtEQVciI+KQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQkOaQJGmSDkr6saxBe6VD+4SkeUnflI8X8ocaLJHyDeM+4G3gaYrv7V+VNGVm37d1vWBmJyqIMWgj5Up7HJgxs5/N7G/gI4qatKAmUqSl1p89U5bvXpS0p0N7kIkUaSn1Z58Ae83sMeAL4IOOA0mTkpqSmvPz8xuLNFgmRVrX+jMz+93M7pSH7wIHOg1kZmfMrGFmjYGBnurpAtKkXQX2S3pY0k5gnKImbZm2GutR4Id8IQbtpJQ6LUo6AXwO9AFnzeyGpJNA08ymgJcljQKLFEX1ExXGvO1RXUsHRvmups2s0cu5kRFxSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSEhzSK76tHskXSjbr0jamzvQYIWUTe+W6tMOAUPAEUlDbd2OAX+a2SPAm8AbuQMNVshVnzbGSqXMReApSZ2qbYIM5KpPW+5jZovAAnB/jgCD1aRsxZVSn5a0h1rr/mnAnbX2Gd0kdgO3a5z/0V5PTJGWsj/aUp9ZSTuA++iwJWXr/mmSmr0WIORgK8zf67lZ6tPK4+fK588CX1pd5TjbgFz1ae8BH0qaobjCxqsMettjZrU8gMm65vY+f21FhUHvRBrLIZVLqzsFVucSUZLOSrq11kebcvPbU2Vs1yQNJw1c8ft2H3AT2AfsBL4Fhtr6vAicLp+PUyzXtJnzTwBvVfTzPwEMA9fXaD8MfEbxOXcEuJIybtVXWt0psFqXiDKzS3T4vNrCGHDOCi4Du9qW9+hI1dLqToFt9SWiUuP7D1VLy5YCq3D+pCWiKqKnn71qaRtJgbFeCqyq+S1xiaiKSPn9rKJqaXWnwLb6ElFTwNHyLnIEWDCzua5nVXn32HKH9BPFXdyr5WsngdHy+b3Ax8AM8DWwb5Pnfx24QXFn+RUwmHHu88Ac8A/FVXUMOA4cL9tF8Q/mm8B3QCNl3MiIOCQyIg4JaQ4JaQ4JaQ4JaQ4JaQ4JaQ4JaQ75F27ynSI7wSrCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a grid of 3x3 images\n",
    "for i in range(0,9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    img = X_train[i].transpose([1,2,0])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing the dataset\n",
    "\n",
    "First things first, we need to preprocess the dataset so the images and labels are in a form that Keras can ingest. To start, we'll define a NumPy seed for reproducibility, then normalize the images. \n",
    "\n",
    "Furthermore, we will also convert our class labels to one-hot vectors.  This is a standard output format for neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a convolutional neural network for object recognition on CIFAR-10\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 6\n",
    "np.random.seed(seed) \n",
    "\n",
    "# load the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize the inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "# class labels shape\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class labels are a single integer value (0-9).  What we really want is a one-hot vector of length ten.  For example, the class label of 6 should be denoted [0, 0, 0, 0, 0, 0, 1, 0, 0, 0].  We can accomplish this using the np_utils.to_categorical() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# hot encode outputs\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = Y_test.shape[1]\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the All-CNN\n",
    "\n",
    "Using the paper as a reference, we can implement the All-CNN network in Keras.  Keras models are built by simply adding layers, one after another. \n",
    "\n",
    "To make things easier for us later, we will wrap this model in a function, which will allow us to quickly and neatly generate the model later on in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start building the model - import necessary layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Conv2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def allcnn(weights=None):\n",
    "    # define model type - Sequential\n",
    "    model = Sequential()\n",
    "\n",
    "    # add model layers - Convolution2D, Activation, Dropout\n",
    "    model.add(Conv2D(96, (3, 3), padding = 'same', input_shape=(3, 32, 32)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(96, (3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(96, (3, 3), padding = 'same', strides = (2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(192, (3, 3), padding = 'same', strides = (2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(192, (3, 3), padding = 'same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(192, (1, 1), padding = 'valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(10, (1, 1), padding = 'valid'))\n",
    "\n",
    "    # add GlobalAveragePooling2D layer with Softmax activation\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # load the weights\n",
    "    if weights:\n",
    "        model.load_weights(weights)\n",
    "    \n",
    "    # return model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Defining Parameters and Training the Model\n",
    "\n",
    "We're all set! We are ready to start training our network.  In the following cells, we will define our hyper parameters, such as learning rate and momentum, define an optimizer, compile the model, and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 3, 32, 96)         27744     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3, 32, 96)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 32, 96)         83040     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 32, 96)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 16, 96)         83040     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 16, 96)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 16, 192)        166080    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 16, 192)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 16, 192)        331968    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 16, 192)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 1, 8, 192)         37056     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 1, 8, 10)          1930      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,394,794\n",
      "Trainable params: 1,394,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have shape (3, 32, 32) but got array with shape (32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e6ef3dfcb4ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have shape (3, 32, 32) but got array with shape (32, 32, 3)"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-6\n",
    "momentum = 0.9\n",
    "\n",
    "# build model \n",
    "model = allcnn()\n",
    "\n",
    "# define optimizer and compile model\n",
    "sgd = SGD(lr=learning_rate, decay=weight_decay, momentum=momentum, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# print model summary\n",
    "print (model.summary())\n",
    "\n",
    "# define additional training parameters\n",
    "epochs = 350\n",
    "batch_size = 32\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Woah, that's a long time...\n",
    "\n",
    "Uh oh. It's apparent that training this deep convolutional neural network is going to take a long time, which is not surprising considering the network has about 1.3 million parameters. Updating this many parameters takes a considerable amount of time; unless, of course, you are using a Graphics Processing Unit (GPU). This is a good time for a quick lesson on the differences between CPUs and GPUs.  \n",
    "\n",
    "The **central processing unit (CPU)** is often called the brains of the PC because it handles the majority of necessary computations. All computers have a CPU and this is what Keras and Theano automatically utilize. \n",
    "\n",
    "The **graphics processing unit (GPU)** is in charge of image rendering.  The most advanced GPUs were originally designed for gamers; however, GPU-accelerated computing, the use of a GPU together with a CPU to accelarate deep learing, analytics, and engineering applications, has become increasingly common.  In fact, the training of deep neural networks is not realistic without them. \n",
    "\n",
    "The most common GPUs for deep learning are produced by NVIDIA.  Furthermore, the NVIDIA Deep Learning SDK provides high-performance tools and libraries to power GPU-accelerated machine learning applications. An alternative would be an AMD GPU in combination with the OpenCL libraries; however, these libraries have fewer active users and less support than the NVIDIA libraries. \n",
    "\n",
    "If your computer has an NVIDIA GPU, installing the CUDA Drivers and CUDA Tookit from NVIDIA will allow Theano and Keras to utilize GPU-accelerated computing. The original paper mentions that it took approximately 10 hours to train the All-CNN network for 350 epochs using a modern GPU, which is considerably faster (several orders of magnitude) than it would take to train on CPU. \n",
    "\n",
    "If you haven't already, stop the cell above. In the following cells, we'll save some time by loading pre-trained weights for the All-CNN network. Using these weights, we can evaluate the performance of the All-CNN network on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'all_cnn_weights_0.9088_0.4994.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5fd36318059f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# define weights and build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all_cnn_weights_0.9088_0.4994.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# define optimizer and compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b75f51e2a83b>\u001b[0m in \u001b[0;36mallcnn\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# load the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# return model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'all_cnn_weights_0.9088_0.4994.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-6\n",
    "momentum = 0.9\n",
    "\n",
    "# define weights and build model\n",
    "weights = 'all_cnn_weights_0.9088_0.4994.hdf5'\n",
    "model = allcnn(weights)\n",
    "\n",
    "# define optimizer and compile model\n",
    "sgd = SGD(lr=learning_rate, decay=weight_decay, momentum=momentum, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# print model summary\n",
    "print (model.summary())\n",
    "\n",
    "# test the model with pretrained weights\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Making Predictions\n",
    "\n",
    "Using the pretrained weights, we were able to achieve an accuracy of nearly 90 percent! Let's leverage this network to make some predictions. To start, we will generate a dictionary of class labels and names by referencing the website for the CIFAR-10 dataset:\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "Next, we'll make predictions on nine images and compare the results to the ground-truth labels.  Furthermore, we will plot the images for visual reference, this is object recognition after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have shape (3, 32, 32) but got array with shape (32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b3f1deced524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have shape (3, 32, 32) but got array with shape (32, 32, 3)"
     ]
    }
   ],
   "source": [
    "# make dictionary of class labels and names\n",
    "classes = range(0,10)\n",
    "\n",
    "names = ['airplane',\n",
    "        'automobile',\n",
    "        'bird',\n",
    "        'cat',\n",
    "        'deer',\n",
    "        'dog',\n",
    "        'frog',\n",
    "        'horse',\n",
    "        'ship',\n",
    "        'truck']\n",
    "\n",
    "# zip the names and classes to make a dictionary of class_labels\n",
    "class_labels = dict(zip(classes, names))\n",
    "\n",
    "# generate batch of 9 images to predict\n",
    "batch = X_test[100:109]\n",
    "labels = np.argmax(Y_test[100:109],axis=-1)\n",
    "\n",
    "# make predictions\n",
    "predictions = model.predict(batch, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(predictions)? (<ipython-input-20-b3e7655a7618>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-b3e7655a7618>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print predictions\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(predictions)?\n"
     ]
    }
   ],
   "source": [
    "# print our predictions\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-47aa23d7a447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# these are individual class probabilities, should sum to 1.0 (100%)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# these are individual class probabilities, should sum to 1.0 (100%)\n",
    "for image in predictions:\n",
    "    print(np.sum(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(class_result)? (<ipython-input-22-32ac5a814ee3>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-32ac5a814ee3>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print class_result\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(class_result)?\n"
     ]
    }
   ],
   "source": [
    "# use np.argmax() to convert class probabilities to class labels\n",
    "class_result = np.argmax(predictions,axis=-1)\n",
    "print class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fc13be9b8f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# determine label for each prediction, set title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mclass_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Prediction: {}\\nActual: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_result' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a grid of 3x3 images\n",
    "fig, axs = plt.subplots(3, 3, figsize = (15, 6))\n",
    "fig.subplots_adjust(hspace = 1)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, img in enumerate(batch):\n",
    "\n",
    "    # determine label for each prediction, set title\n",
    "    for key, value in class_labels.items():\n",
    "        if class_result[i] == key:\n",
    "            title = 'Prediction: {}\\nActual: {}'.format(class_labels[key], class_labels[labels[i]])\n",
    "            axs[i].set_title(title)\n",
    "            axs[i].axes.get_xaxis().set_visible(False)\n",
    "            axs[i].axes.get_yaxis().set_visible(False)\n",
    "            \n",
    "    # plot the image\n",
    "    axs[i].imshow(img.transpose([1,2,0]))\n",
    "    \n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
